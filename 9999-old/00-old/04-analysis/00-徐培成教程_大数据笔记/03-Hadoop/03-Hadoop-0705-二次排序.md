* `参考  290/756  [O'REILLY]Hadoop.The.Definitive.Guide.4th.Edition.2015.3`

*简单说一下：二次排序其实就是对value进行排序，但是value又不能排序。所以我们可以自定义一个模型，把value放在模型中，然后把模型作在map中映射成key，这样子就可以对value进行排序了*

*下面的二次排序已经可以很大程度上解决reducer端的数据计算问题，因为这里到reducer已经排好序，直接取出来就好了。先这么说明，好像也有点不对*

*?突然想到一个问题：二次排序如何避免数据倾斜呢？*

## 二次排序

### 1,先定义模型，也就是组合key，value放在其中

```java
package im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-25 10:45
 * #description : 自定义key
 *              : 二次排序的核心就在这里，通过自定义的key，把value隐藏在其中，就能实现对value的排序
 **/
public class CombinedKey implements WritableComparable<CombinedKey> {

    private int key;

    private int value;

    //这里比较的意思就是年份升序，气温降序
    public int compareTo(CombinedKey o) {

        int otherKey = o.getKey();
        int otherValue = o.getValue();

        if (key != otherKey) {
            return (key - otherKey);
        }else {
            return -(value - otherValue);
        }
    }

    /*public int compareTo(Object o) {
        return this.compareTo((CombinedKey) o);
    }*/

    public void write(DataOutput out) throws IOException {
        out.writeInt(key);
        out.writeInt(value);
    }

    public void readFields(DataInput in) throws IOException {
        key = in.readInt();
        value = in.readInt();
    }

    public int getKey() {
        return key;
    }

    public void setKey(int key) {
        this.key = key;
    }

    public int getValue() {
        return value;
    }

    public void setValue(int value) {
        this.value = value;
    }
}

```

### 2, 我们的mapper如下

```java
package im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 18:50
 * #description :
 **/
public class IMMaxTempMapper extends Mapper<IntWritable, IntWritable, CombinedKey, NullWritable> {

    @Override
    protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException {
        System.out.println("key:" + key + ",value:" + value);

        CombinedKey combinedKey = new CombinedKey();
        combinedKey.setKey(key.get());
        combinedKey.setValue(value.get());
        context.write(combinedKey, NullWritable.get());
    }

}
```

### 3, 然后是分区类

```java
package im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Partitioner;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 21:31
 * #description : 根据年份进行分区,因为是对mapper的输出进行分区，所以这里的输入就应该是mapper的输出
 **/
public class IMYearPatitioner extends Partitioner<CombinedKey, NullWritable> {

    public int getPartition(CombinedKey key, NullWritable value, int i) {

        int year = key.getKey();

        if (year < 2000) {
            return 0;
        } else if (year < 2030) {
            return 1;
        } else {
            return 2;
        }
    }
}
```

### 4, 模型的比较方法需要自己写一下，默认情况下key不是排序的，所以我们需要先进行排序。然后才是分组

```java
package im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-25 12:24
 * #description : 用来比较两个CombinedKey的顺序
 *              : 这个类的作用是为了设置到job中去， 因为对比器你又不能直接用模型中的方法，所以需要重新定义一个类放进去
 **/
public class IMCombinedKeyComparator extends WritableComparator {

    protected IMCombinedKeyComparator() {
        super(CombinedKey.class, true);
    }

    @Override
    public int compare(WritableComparable w1, WritableComparable w2) {
    
        CombinedKey key01 = (CombinedKey) w1;
        CombinedKey key02 = (CombinedKey) w2;

        return key01.compareTo(key02);
    }
}
```

### 5, 重新分组方法，同一年的需要放在同一组中

```java
package im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-25 12:16
 * #description : 分组
 **/
public class IMYearGroupSeparator extends WritableComparator {

    protected IMYearGroupSeparator() {
        super(CombinedKey.class, true);
    }

    @Override
    public int compare(WritableComparable w1, WritableComparable w2) {

        /*IntPair ip1 = (IntPair) w1;
        IntPair ip2 = (IntPair) w2;
        return IntPair.compare(ip1.getFirst(), ip2.getFirst());*/

        CombinedKey key01 = (CombinedKey) w1;
        CombinedKey key02 = (CombinedKey) w2;

        //因为分组的话，只能根据年份分组，也就是CombinedKey的key值进行分组
        return (key01.getKey() - key02.getKey());
    }

}
```

### 6, reducer

```java
package im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 18:51
 * #description :
 **/
public class IMMaxTempReducer extends Reducer<CombinedKey, NullWritable, IntWritable, IntWritable> {

    @Override
    protected void reduce(CombinedKey key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {

        int year = key.getKey();
        int temp = key.getValue();

        System.out.println("-----------------reducer-----------------");

        for (NullWritable nullWritable : values) {
            System.out.println("year: " + key.getKey() + ", temp: " + key.getValue());
        }
        context.write(new IntWritable(year), new IntWritable(temp));
    }
}
```

### 7, 最后设置app就好

```java
package im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 18:53
 * #description :
 **/

//-------------------二次排序是不是不能使用采样？如果使用采样，怎么能定义一下采样的

public class IMMaxTempApp {

    public static void main(String[] args) throws Exception {

        //1，先判断参数，如果通过则取出参数
        if (args.length != 2) {
            System.out.println("参数个数有误！");
            return;
        }
        String inputFileStr = args[0];
        String outputFolderStr = args[1];

        //2，获取文件系统，并删除输出文件夹以免出错
        Configuration configuration = new Configuration();
        FileSystem fileSystem = FileSystem.get(configuration);
        fileSystem.delete(new Path(outputFolderStr));

        //3,创建作业, 并设置基本的选项
        Job maxTempJob = Job.getInstance(configuration);
        maxTempJob.setJobName("maxTempJob");
        maxTempJob.setJarByClass(IMMaxTempApp.class);
        //因为采用的是序列文件，所以这里需要设置一下
        maxTempJob.setInputFormatClass(SequenceFileInputFormat.class);

        //4, 输入
        FileInputFormat.addInputPath(maxTempJob, new Path(inputFileStr));


        //5, 设置mapper
        maxTempJob.setMapperClass(IMMaxTempMapper.class);
        //这里其实是有点蠢的，其实在IMMaxTempMapper中已经指定了输出的key和value的类型了，但是这里还是需要指定一下，要不然就有可能不对
        maxTempJob.setMapOutputKeyClass(CombinedKey.class);
        maxTempJob.setMapOutputValueClass(NullWritable.class);

        //6, 设置reducer
        //注意：如果有多个r的话(一般都会有多个r)，那么这样的就不属于全排序了，因为不同的年份会分布在不同的r中处理，也就会有多个文件输出
        maxTempJob.setReducerClass(IMMaxTempReducer.class);
        maxTempJob.setNumReduceTasks(3);

        //设置分区函数，自定义分区是ok的，那我们再试试用全排序加采样进行试试
        maxTempJob.setPartitionerClass(IMYearPatitioner.class);


        //排序和分组都是在mapper之后进行的，先进行排序，排序之后在进行分组

        //设置排序
        maxTempJob.setSortComparatorClass(IMCombinedKeyComparator.class);

        //设置分组
        maxTempJob.setGroupingComparatorClass(IMYearGroupSeparator.class);

        //------------------------注意：采样要放在分区之后，否则会报错什么找不到分区文件，这种问题其实感觉应该是bug，不然为啥要放在后面，本来采样就是在r之前的啊------------------------------------
        /*

        //7.1, 设置分区函数,这里还是不能用自己定义的分区函数，因为你用自己定义的分区函数，采样的意义也就没有了
        //maxTempJob.setPartitionerClass(IMYearPatitioner.class);
        maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);
        //如果需要指定位置的话，需要注意一下：设置的时候使用configuration是不能直接使用的，而是需要从maxTempJob.getConfiguration()取出来使用，因为job里面是重新拷贝了一份
        //TotalOrderPartitioner.setPartitionFile(maxTempJob.getConfiguration(), new Path("/Users/ivanl001/Desktop/bigData/output005/partitioner.lst"));

        //7.2, 设置采样
        //说明一下，因为采样是对key进行采样的，所以只好用seqfile，要不然比如说用的是文本文件格式， key是偏移量，采样就没有什么意义了
        //这句话暂时不知道为什么要设置，先注释
        //maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);
        InputSampler.Sampler<IntWritable, IntWritable> sampler = new InputSampler.RandomSampler<IntWritable, IntWritable>(0.1, 10000, 10);
        InputSampler.writePartitionFile(maxTempJob, sampler);

        //----------呀呀呀呀，采样真的很好用哈，哈哈哈哈---------------

        *//*Configuration conf = maxTempJob.getConfiguration();
        String partitionFile = TotalOrderPartitioner.getPartitionFile(conf);
        URI partitionUri = new URI(partitionFile);
        maxTempJob.addCacheFile(partitionUri);*//*
        */

        //7，输出
        FileOutputFormat.setOutputPath(maxTempJob, new Path(outputFolderStr));

        //8, 提交，开始处理
        maxTempJob.waitForCompletion(false);
    }
}
```