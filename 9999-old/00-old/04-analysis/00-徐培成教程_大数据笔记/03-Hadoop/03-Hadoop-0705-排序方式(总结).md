# 排序的n中方式：

## 1, 设定一个Reducer

- 全排序的第一种方式：直接把r的个数改成1个，这个小数据量是合适的，数据量大的话无异于找死

`代码参考：im.ivanl001.bigData.a02_Hadoop_MR.A10_01_maxTemp_partial_sort`



## 2, 自定义分区函数

* 自定义分区函数，合理的把数据平均分到reducers上去
* 比如有3个reducer，100个数据， 分别是0-100，那么在分区函数中：值是：0-33 的返回0，也就是分配到分区0；34-66的返回1，也就是分配到分区1；67-100的返回2，也就是分配到分区2上。

`代码参考：im.ivanl001.bigData.a02_Hadoop_MR.A11_01_maxTemp_total_sort`

### 2.1, 自定义分区函数范例IMYearPatitioner

```java
package im.ivanl001.bigData.a02_Hadoop_MR.A11_02_maxTemp_total_sort;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Partitioner;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 21:31
 * #description : 根据年份进行分区,因为是对mapper的输出进行分区，所以这里的输入就应该是mapper的输出
 **/
public class IMYearPatitioner extends Partitioner<IntWritable, IntWritable> {
    public int getPartition(IntWritable key, IntWritable value, int i) {
      	//可以对key，也可以对value，这里是seqfile，key有序，所以这里直接对key进行判断分区
        if (key.get() < 2000) {
            return 0;
        } else if (key.get() < 2030) {
            return 1;
        } else {
            return 2;
        }
    }
}
```

### 2.2, 自定义分区函数使用

```java
package im.ivanl001.bigData.a02_Hadoop_MR.A11_02_maxTemp_total_sort;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 18:53
 * #description : 全排序的第二种方式：自定义分区函数，比如我们有三个reducer，可以把前1/3进入第一个，最后1/3进入最后一个，其他的进入中间的分区，那么这样就算是有三个文件，只需要把文件进行简单的拼接就可以有正确的全排序了，如下：
 * 但是这样有时候也就有一个问题：如果数据量偏向某一年，那么就咩有特别好的办法防止数据倾斜，这个时候可以用数据采样，看下一个包
 **/
public class IMMaxTempApp {

    public static void main(String[] args) throws Exception {


        //1，先判断参数，如果通过则取出参数
        if (args.length != 2) {
            System.out.println("参数个数有误！");
            return;
        }
        String inputFileStr = args[0];
        String outputFolderStr = args[1];


        //2，获取文件系统，并删除输出文件夹以免出错
        Configuration configuration = new Configuration();
        FileSystem fileSystem = FileSystem.get(configuration);
        fileSystem.delete(new Path(outputFolderStr));


        //3,创建作业, 并设置基本的选项
        Job maxTempJob = Job.getInstance(configuration);
        maxTempJob.setJobName("maxTempJob");
        maxTempJob.setJarByClass(IMMaxTempApp.class);

        //4, 输入
        FileInputFormat.addInputPath(maxTempJob, new Path(inputFileStr));


        //5, 设置mapper
        maxTempJob.setMapperClass(IMMaxTempMapper.class);
        //这里其实是有点蠢的，其实在IMMaxTempMapper中已经指定了输出的key和value的类型了，但是这里还是需要指定一下，要不然就有可能不对
        maxTempJob.setMapOutputKeyClass(IntWritable.class);
        maxTempJob.setMapOutputValueClass(IntWritable.class);


        //5.1, 设置分区函数
        maxTempJob.setPartitionerClass(IMYearPatitioner.class);


        //6, 设置reducer
        //注意：如果有多个r的话(一般都会有多个r)，那么这样的就不属于全排序了，因为不同的年份会分布在不同的r中处理，也就会有多个文件输出
        maxTempJob.setReducerClass(IMMaxTempReducer.class);
        maxTempJob.setNumReduceTasks(3);

        //7，输出
        FileOutputFormat.setOutputPath(maxTempJob, new Path(outputFolderStr));

        //8, 提交，开始处理
        maxTempJob.waitForCompletion(false);
    }
}
```







## 3, 默认的全排序函数(需采样)

`代码参考：im.ivanl001.bigData.a02_Hadoop_MR.A11_03_maxTemp_total_sort`

* 也就是不自定义分区函数，而是使用默认的TotalOrderPartitioner
* TotalOrderPartitioner使用的时候需要对数据进行采样
* 好像这种方式只适合seq文件，不确定



### 3.1, 全排序使用



```java
package im.ivanl001.bigData.a02_Hadoop_MR.A11_03_maxTemp_total_sort;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.partition.InputSampler;
import org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner;


/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 18:53
 * #description : 全排序的第三种方式：默认的全局排序函数：TotalOrderPartitioner， 排序的时候需要采样
 **/
public class IMMaxTempApp {

    public static void main(String[] args) throws Exception {

        //1，先判断参数，如果通过则取出参数
        if (args.length != 2) {
            System.out.println("参数个数有误！");
            return;
        }
        String inputFileStr = args[0];
        String outputFolderStr = args[1];


        //2，获取文件系统，并删除输出文件夹以免出错
        Configuration configuration = new Configuration();
        FileSystem fileSystem = FileSystem.get(configuration);
        fileSystem.delete(new Path(outputFolderStr));


        //3,创建作业, 并设置基本的选项
        Job maxTempJob = Job.getInstance(configuration);
        maxTempJob.setJobName("maxTempJob");
        maxTempJob.setJarByClass(IMMaxTempApp.class);
        //因为采用的是序列文件，所以这里需要设置一下
        maxTempJob.setInputFormatClass(SequenceFileInputFormat.class);


        //4, 输入
        FileInputFormat.addInputPath(maxTempJob, new Path(inputFileStr));


        //5, 设置mapper
        maxTempJob.setMapperClass(IMMaxTempMapper.class);
        //这里其实是有点蠢的，其实在IMMaxTempMapper中已经指定了输出的key和value的类型了，但是这里还是需要指定一下，要不然就有可能不对
        maxTempJob.setMapOutputKeyClass(IntWritable.class);
        maxTempJob.setMapOutputValueClass(IntWritable.class);


        //6, 设置reducer
        //注意：如果有多个r的话(一般都会有多个r)，那么这样的就不属于全排序了，因为不同的年份会分布在不同的r中处理，也就会有多个文件输出
        maxTempJob.setReducerClass(IMMaxTempReducer.class);
        maxTempJob.setNumReduceTasks(3);

        //------------------------注意：采样要放在分区之后，否则会报错什么找不到分区文件，这种问题其实感觉应该是bug，不然为啥要放在后面，本来采样就是在r之前的啊------------------------------------

        //7.1, 设置分区函数,这里还是不能用自己定义的分区函数，因为你用自己定义的分区函数，采样的意义也就没有了
        //maxTempJob.setPartitionerClass(IMYearPatitioner.class);
        maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);
        //如果需要指定位置的话，需要注意一下：设置的时候使用configuration是不能直接使用的，而是需要从maxTempJob.getConfiguration()取出来使用，因为job里面是重新拷贝了一份
        //TotalOrderPartitioner.setPartitionFile(maxTempJob.getConfiguration(), new Path("/Users/ivanl001/Desktop/bigData/output005/partitioner.lst"));

        //7.2, 设置采样
        //说明一下，因为采样是对key进行采样的，所以只好用seqfile，要不然比如说用的是文本文件格式， key是偏移量，采样就没有什么意义了
        //这句话暂时不知道为什么要设置，先注释
        //maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);
        InputSampler.Sampler<IntWritable, IntWritable> sampler = new InputSampler.RandomSampler<IntWritable, IntWritable>(0.1, 10000, 10);
        InputSampler.writePartitionFile(maxTempJob, sampler);

        //----------呀呀呀呀，采样真的很好用哈，哈哈哈哈---------------
        /*Configuration conf = maxTempJob.getConfiguration();
        String partitionFile = TotalOrderPartitioner.getPartitionFile(conf);
        URI partitionUri = new URI(partitionFile);
        maxTempJob.addCacheFile(partitionUri);*/

        //7，输出
        FileOutputFormat.setOutputPath(maxTempJob, new Path(outputFolderStr));

        //8, 提交，开始处理
        maxTempJob.waitForCompletion(false);
    }
}
```





## 4, 二次排序, 也就是组合key

`全部代码参考：im.ivanl001.bigData.a02_Hadoop_MR.A12_maxTemp_2nd_sort`



* 简单来讲，就是0: 把value也放进key
* 然后因为key不是一个字符串了，那么就不能通过默认的hash分区了
* 所以还需要：
  * 1,  自定义分区函数
  * 2, 自定义排序函数
  * 3, 自定义分组函数



### 4.0, app类

```java
package im.ivanl001.bigData.a02_Hadoop_MR.A12_maxTemp_2nd_sort;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 18:53
 * #description :  二次排序不是有两层reducer，而是把key是第一层排序，value是第二层排序，然后像气温这个案例，二次排序后，年份分区，然后选择对应年份的第一个数据也就是最高或者最低气温了
 **/

//-------------------二次排序是不是不能使用采样？如果使用采样，怎么能定义一下采样的

public class IMMaxTempApp {

    public static void main(String[] args) throws Exception {


        //1，先判断参数，如果通过则取出参数
        if (args.length != 2) {
            System.out.println("参数个数有误！");
            return;
        }
        String inputFileStr = args[0];
        String outputFolderStr = args[1];


        //2，获取文件系统，并删除输出文件夹以免出错
        Configuration configuration = new Configuration();
        FileSystem fileSystem = FileSystem.get(configuration);
        fileSystem.delete(new Path(outputFolderStr));


        //3,创建作业, 并设置基本的选项
        Job maxTempJob = Job.getInstance(configuration);
        maxTempJob.setJobName("maxTempJob");
        maxTempJob.setJarByClass(IMMaxTempApp.class);
        //因为采用的是序列文件，所以这里需要设置一下
        maxTempJob.setInputFormatClass(SequenceFileInputFormat.class);


        //4, 输入
        FileInputFormat.addInputPath(maxTempJob, new Path(inputFileStr));


        //5, 设置mapper
        maxTempJob.setMapperClass(IMMaxTempMapper.class);
        //这里其实是有点蠢的，其实在IMMaxTempMapper中已经指定了输出的key和value的类型了，但是这里还是需要指定一下，要不然就有可能不对
        maxTempJob.setMapOutputKeyClass(CombinedKey.class);
        maxTempJob.setMapOutputValueClass(NullWritable.class);


        //6, 设置reducer
        //注意：如果有多个r的话(一般都会有多个r)，那么这样的就不属于全排序了，因为不同的年份会分布在不同的r中处理，也就会有多个文件输出
        maxTempJob.setReducerClass(IMMaxTempReducer.class);
        maxTempJob.setNumReduceTasks(3);


        //设置分区函数，因为我们的key是组合key，直接对组合key进行hash分区肯定是不对的了。那么就自己设定分区函数，下面分组也是一样
        //设置分区函数，自定义分区是ok的，那我们再试试用全排序加采样进行试试
        maxTempJob.setPartitionerClass(IMYearPatitioner.class);


        //排序和分组都是在mapper之后进行的，先进行排序，排序之后在进行分组

        //设置排序
        maxTempJob.setSortComparatorClass(IMCombinedKeyComparator.class);

        //设置分组，因为我们的key是组合key，直接对组合key进行分组肯定不会了。那么就自己设定
        maxTempJob.setGroupingComparatorClass(IMYearGroupSeparator.class);




        //------------------------注意：采样要放在分区之后，否则会报错什么找不到分区文件，这种问题其实感觉应该是bug，不然为啥要放在后面，本来采样就是在r之前的啊------------------------------------
        /*

        //7.1, 设置分区函数,这里还是不能用自己定义的分区函数，因为你用自己定义的分区函数，采样的意义也就没有了
        //maxTempJob.setPartitionerClass(IMYearPatitioner.class);
        maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);
        //如果需要指定位置的话，需要注意一下：设置的时候使用configuration是不能直接使用的，而是需要从maxTempJob.getConfiguration()取出来使用，因为job里面是重新拷贝了一份
        //TotalOrderPartitioner.setPartitionFile(maxTempJob.getConfiguration(), new Path("/Users/ivanl001/Desktop/bigData/output005/partitioner.lst"));

        //7.2, 设置采样
        //说明一下，因为采样是对key进行采样的，所以只好用seqfile，要不然比如说用的是文本文件格式， key是偏移量，采样就没有什么意义了
        //这句话暂时不知道为什么要设置，先注释
        //maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);
        InputSampler.Sampler<IntWritable, IntWritable> sampler = new InputSampler.RandomSampler<IntWritable, IntWritable>(0.1, 10000, 10);
        InputSampler.writePartitionFile(maxTempJob, sampler);

        //----------呀呀呀呀，采样真的很好用哈，哈哈哈哈---------------

        *//*Configuration conf = maxTempJob.getConfiguration();
        String partitionFile = TotalOrderPartitioner.getPartitionFile(conf);
        URI partitionUri = new URI(partitionFile);
        maxTempJob.addCacheFile(partitionUri);*//*

        */


        //7，输出
        FileOutputFormat.setOutputPath(maxTempJob, new Path(outputFolderStr));

        //8, 提交，开始处理
        maxTempJob.waitForCompletion(false);
    }
}
```



### 4.1, mapper类

```java
package im.ivanl001.bigData.a02_Hadoop_MR.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 18:50
 * #description :
 **/
public class IMMaxTempMapper extends Mapper<IntWritable, IntWritable, CombinedKey, NullWritable> {

    @Override
    protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException {
        System.out.println("key:" + key + ",value:" + value);

        //对于组合key，内部会根据compareTo排序输出
        CombinedKey combinedKey = new CombinedKey();
        combinedKey.setKey(key.get());
        combinedKey.setValue(value.get());
        context.write(combinedKey, NullWritable.get());
    }
}
```



### 4.2, 组合key模型：CombinedKey

```java
package im.ivanl001.bigData.a02_Hadoop_MR.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-25 10:45
 * #description : 自定义key
 *              : 二次排序的核心就在这里，通过自定义的key，把value隐藏在其中，就能实现对value的排序
 **/
public class CombinedKey implements WritableComparable<CombinedKey> {

    private int key;

    private int value;

    //这里比较的意思就是年份升序，气温降序
    public int compareTo(CombinedKey o) {

        int otherKey = o.getKey();
        int otherValue = o.getValue();

        //年份不同，按照年份排序即可
        if (key != otherKey) {
            return (key - otherKey);
        }else {//年份相同，按照值也就是value排序
            return -(value - otherValue);
        }
    }

    /*public int compareTo(Object o) {
        return this.compareTo((CombinedKey) o);
    }*/

    public void write(DataOutput out) throws IOException {
        out.writeInt(key);
        out.writeInt(value);
    }

    public void readFields(DataInput in) throws IOException {
        key = in.readInt();
        value = in.readInt();
    }

    public int getKey() {
        return key;
    }

    public void setKey(int key) {
        this.key = key;
    }

    public int getValue() {
        return value;
    }

    public void setValue(int value) {
        this.value = value;
    }
}
```


### 4.3, 分区函数：IMYearPatitioner

```java
package im.ivanl001.bigData.a02_Hadoop_MR.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Partitioner;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 21:31
 * #description : 根据年份进行分区,因为是对mapper的输出进行分区，所以这里的输入就应该是mapper的输出
 **/
public class IMYearPatitioner extends Partitioner<CombinedKey, NullWritable> {

    public int getPartition(CombinedKey key, NullWritable value, int i) {

        int year = key.getKey();

        if (year < 2000) {
            return 0;
        } else if (year < 2030) {
            return 1;
        } else {
            return 2;
        }
    }
}
```



### 4.4, 排序函数：IMCombinedKeyComparator

```java
package im.ivanl001.bigData.a02_Hadoop_MR.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-25 12:24
 * #description : 用来比较两个CombinedKey的顺序
 *              : 这个类的作用是为了设置到job中去， 因为对比器你又不能直接用模型中的方法，所以需要重新定义一个类放进去
 **/
public class IMCombinedKeyComparator extends WritableComparator {

    protected IMCombinedKeyComparator() {
        super(CombinedKey.class, true);
    }

    @Override
    public int compare(WritableComparable w1, WritableComparable w2) {

        CombinedKey key01 = (CombinedKey) w1;
        CombinedKey key02 = (CombinedKey) w2;

        return key01.compareTo(key02);
    }
}
```



### 4.5, 分组函数：IMYearGroupSeparator

```java
package im.ivanl001.bigData.a02_Hadoop_MR.A12_maxTemp_2nd_sort;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-25 12:16
 * #description : 分组
 **/
public class IMYearGroupSeparator extends WritableComparator {

    protected IMYearGroupSeparator() {
        super(CombinedKey.class, true);
    }

    @Override
    public int compare(WritableComparable w1, WritableComparable w2) {

        /*IntPair ip1 = (IntPair) w1;
        IntPair ip2 = (IntPair) w2;
        return IntPair.compare(ip1.getFirst(), ip2.getFirst());*/

        CombinedKey key01 = (CombinedKey) w1;
        CombinedKey key02 = (CombinedKey) w2;

        //因为分组的话，只能根据年份分组，也就是CombinedKey的key值进行分组
        //年份相同，就分到一组， 一个分区中会有多个组
        return (key01.getKey() - key02.getKey());
    }
}
```





## 5, 组合排序

* 可以组合1和2
* 我先把key添加随机前缀，然后计算之后输出
* 再使用另外一个应用，读取结果，设定一个reducer即可