*随机分区比较好理解：大概意思就是我第一次重写分区类，通过随机值分区，而不是hash或者其他有规律对值。这样子第一次分区并且reduce后，结果是稍微混乱对，但是已经聚合过一次，数据量小很多。然后对输出的文件重新进行MR，这次就按照正常的hash分区然后聚合就可以了*

* 我这里只贴一下第一次的随机分区类和第二次的MR，因为第二次的输入不再是普通的文本输入了。具体如下：

## 第一次MR的分区类

```java
package im.ivanl001.bigData.Hadoop.A13_01_RandomPartitioner;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;

import java.util.Random;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-24 15:09
 * #description : 自定义分区函数
 **/
public class IMPartitioner extends Partitioner<Text, IntWritable>{

    public int getPartition(Text text, IntWritable intWritable, int i) {
        //i这里传进来的是reducer的个数
        return new Random().nextInt(i);
    }
}
```

------------------------------剩下是第二次MR的代码哈------------------------------

## 第二次MR的App代码

```java
package im.ivanl001.bigData.Hadoop.A13_02_RandomPartitioner_stage01;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-20 19:35
 * #description : wordcount
 **/
public class IMWordCountApp {

    /*
     * mapper过程之后产生的文件的命名中是***-m-00000*什么的，m代表是mapper，后面的数字代表是分区
     * reducer过程之后产生的文件的命名中是***-r-00000*什么的，r代表的是reducer，后面的数字代表的也是分区
     * 如果设置三个reducer，在没有重写分区函数的情况下，会有三个r，也就会有三个输出文件，因为一个reducer会有一个输出文件
     * 如果重写了分区函数，其实也会生成三个文件，但是只有算法中有指向的才会有内容，其他的就是空文件了
     * */
    public static void main(String[] args) {

        try {

            if (args.length != 1) {
                System.out.println("参数个数有误！");
                return;
            }

            //"/users/ivanl001/Desktop/bigData/input/zhang.txt"
//            String inputFileStr = args[0];
            String outputFolderStr = args[0];

            //0，创建配置对象，以修正某些配置文件中的配置
            Configuration configuration = new Configuration();
            //这里一旦设置单机版就会出错，而且不能有core-default.xml文件，这个文件中一旦配置也会有问题，不知道为啥，先过
//            configuration.set("fs.defaultFS", "file:///");
            //configuration.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
            //configuration.set("fs.file.impl", org.apache.hadoop.fs.LocalFileSystem.class.getName());

            //这里因为文件存在，总是需要删除，麻烦，所以直接程序自动删除
            FileSystem.get(configuration).delete(new Path(outputFolderStr));


            //1，创建作业
            Job wordcountJob = Job.getInstance(configuration);
            wordcountJob.setJobName("wordcountApp");
            //之前这句没写，就会一直报错，什么mapper类找不到，这里需要注意一下
            wordcountJob.setJarByClass(IMWordCountApp.class);

            //--------------------------------这里设置输出格式类------------------------------------
            //这个是设置输出格式为------------序列文件输出格式-------------，我这里并不想保存序列文件，所以这里就不设置
            //wordcountJob.setOutputFormatClass(SequenceFileOutputFormat.class);


            //2,设置作业输入
            //这句话可以不加，因为默认就是文本输入格式
            //###################################################这里需要注意一下：这里输入格式不能再是TextInputFormat#################################################################
            wordcountJob.setInputFormatClass(KeyValueTextInputFormat.class);
            FileInputFormat.addInputPath(wordcountJob, new Path("/Users/ivanl001/Desktop/bigData/output008/part-r-00000"));
            FileInputFormat.addInputPath(wordcountJob, new Path("/Users/ivanl001/Desktop/bigData/output008/part-r-00001"));
            FileInputFormat.addInputPath(wordcountJob, new Path("/Users/ivanl001/Desktop/bigData/output008/part-r-00002"));
            FileInputFormat.addInputPath(wordcountJob, new Path("/Users/ivanl001/Desktop/bigData/output008/part-r-00003"));



            //3，设置mapper
            wordcountJob.setMapperClass(IMWordCountMapper.class);
            wordcountJob.setMapOutputKeyClass(Text.class);
            wordcountJob.setMapOutputValueClass(IntWritable.class);



            //----中间设置一下分区函数
            //这里是第二个阶段，所以不需要进行分区了，直接读取，然后做正确的hash分区即可
            //wordcountJob.setPartitionerClass(IMPartitioner.class);



            //4, 设置reducer
            wordcountJob.setReducerClass(IMWordCountReducer.class);
            //每个reduce会产生一个输出结果或者输出文件，这里设置一个reduce
            wordcountJob.setNumReduceTasks(2);//设置reducer的个数，如果是0就是不需要r
            //设置输出的key和value的类型
            wordcountJob.setOutputKeyClass(Text.class);
            wordcountJob.setOutputValueClass(IntWritable.class);

            //5, 设置输出
            wordcountJob.setOutputValueClass(FileOutputFormat.class);
            FileOutputFormat.setOutputPath(wordcountJob, new Path(outputFolderStr));

            //6，提交，开始处理
            wordcountJob.waitForCompletion(false);

        } catch (IOException e) {
            e.printStackTrace();
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}
```

## 第二次MR的Mapper

```java
package im.ivanl001.bigData.Hadoop.A13_02_RandomPartitioner_stage01;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-20 19:23
 * #description : mapper
 **/
public class IMWordCountMapper extends Mapper<Text, Text, Text, IntWritable>{

    @Override
    protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {

        //因为输入格式是：KeyValueTextInputFormat，而这个job作业是上一次随机分区作业完成之后进行的另外一个作业，所以输入也就是上一个作业产生的文件，内容大致如下：所以key直接就是Text，value是IntWritable，可以直接写出到r中去
        /*
        Because	1
        Do	1
         */

        context.write(key, new IntWritable(Integer.parseInt(value.toString())));

        /*System.out.println("key:" + key + ",value:" + value);
        String[] splitStr = value.toString().split(" ");
        Text outText = new Text();
        IntWritable outInt = new IntWritable();
        for (String str : splitStr) {
            outText.set(str);
            outInt.set(1);
            //这里是意思就是把每个单词拼成(zhang,1), (li, 1), (dan, 1)类似的格式传给reduce
            context.write(outText, outInt);
        }*/
    }
}
```

## 第二次MR的Reducer

```java
package im.ivanl001.bigData.Hadoop.A13_02_RandomPartitioner_stage01;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-10-20 19:30
 * #description : reducer
 **/
public class IMWordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {


    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int count = 0;
        for (IntWritable intWritable : values) {
            count = count + intWritable.get();
        }
        context.write(key, new IntWritable(count));
    }
}
```



