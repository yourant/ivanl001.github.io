> 1，一张大表，一张小表，Mapper端连接

>2的join先放着，后面有用到再看

## 1，一张大表，一张小表，Mapper端连接
*用来右连接的mapper, 如果两个表中有一个比较小，可以放入内存，那么我们可以直接把另外一张表mapper，小表放入内存，然后进行连接，代码如下.因为只是演示，所以这里只写了一个Mapper和主程序app*

### 1.1, Mapper，这个类用来加载小表，并进行连接
```java
package im.ivanl001.bigData.Hadoop.A17_MapperJoin;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashMap;
import java.util.Map;

//mapreduce是新的接口中的一个包， mapred是老接口的一个包

/**
 * #author      : ivanl001
 * #creator     : 2018-11-01 17:47
 * #description : 用来右连接的mapper, 如果两个表中有一个比较小，可以放入内存，那么我们可以直接把另外一张表mapper，小表放入内存，然后进行连接，代码如下
 **/
public class IMJoinMapper extends Mapper<LongWritable, Text, Text, NullWritable> {

    private Map<String, String> users = new HashMap<String, String>();

    //这个方法是map之前会调用的一个方法，其他的准备工作可以在这里完成
    //我们需要在这里加载小表，并放入到mapper里面
    @Override
    protected void setup(Context context) throws IOException, InterruptedException {

        Configuration configuration = context.getConfiguration();
        FileSystem fileSystem = FileSystem.getLocal(configuration);

        FSDataInputStream fsDataInputStream = fileSystem.open(new Path("/Users/ivanl001/Desktop/bigData/join/customers.txt"));

        //得到字符阅读器
        BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(fsDataInputStream));

        String line = null;
        while ((line = bufferedReader.readLine()) != null) {
            String cid = line.substring(0, line.indexOf(","));
            String userInfo = line;
            users.put(cid, userInfo);
        }
        //这里在map开始run之前已经把用户信息读入到内存中了，very good

    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

        /*String[] splitStr = value.toString().split(",");
        String cid = splitStr[splitStr.length];*/

        String line = value.toString();

        //截取最后一个逗号后面的用户id
        String cid = line.substring(line.lastIndexOf(",") + 1);//默认到最后

        //截取最用一个逗号之前的用户信息
        String orderInfo = line.substring(0, line.lastIndexOf(","));

        //通过订单中的cid获取内存中的用户信息，拼接到一起
        String userInfo = users.get(cid);
        String joinInfo = userInfo + "----" + orderInfo;

        context.write(new Text(joinInfo), NullWritable.get());
    }
}
```

### 1.2，App

```java
package im.ivanl001.bigData.Hadoop.A17_MapperJoin;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-11-01 18:16
 * #description : 这里直接通过mapper进行join
 **/
public class IMMapperJoinApp {

    public static void main(String[] args) {

        try {
            if (args.length != 2) {
                System.out.println("参数个数有误！");
                return;
            }

            //"/users/ivanl001/Desktop/bigData/input/zhang.txt"
            String inputFileStr = args[0];
            String outputFolderStr = args[1];

            //0，创建配置对象，以修正某些配置文件中的配置
            Configuration configuration = new Configuration();
            //这里一旦设置单机版就会出错，而且不能有core-default.xml文件，这个文件中一旦配置也会有问题，不知道为啥，先过
//            configuration.set("fs.defaultFS", "file:///");
            //configuration.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
            //configuration.set("fs.file.impl", org.apache.hadoop.fs.LocalFileSystem.class.getName());

            //这里因为文件存在，总是需要删除，麻烦，所以直接程序自动删除
            FileSystem.get(configuration).delete(new Path(outputFolderStr));


            //1，创建作业
            Job wordcountJob = Job.getInstance(configuration);
            wordcountJob.setJobName("wordcountApp");
            //之前这句没写，就会一直报错，什么mapper类找不到，这里需要注意一下
            wordcountJob.setJarByClass(IMMapperJoinApp.class);


            //2,设置作业输入
            //这句话可以不加，因为默认就是文本输入格式
            wordcountJob.setInputFormatClass(TextInputFormat.class);
            FileInputFormat.addInputPath(wordcountJob, new Path(inputFileStr));

            //3，设置mapper
            wordcountJob.setMapperClass(IMJoinMapper.class);
            wordcountJob.setMapOutputKeyClass(Text.class);
            wordcountJob.setMapOutputValueClass(NullWritable.class);

            //4, 设置reducer,这里不需要r，所以就直接全部忽略


            //5, 设置输出
            //wordcountJob.setOutputValueClass(FileOutputFormat.class);
            FileOutputFormat.setOutputPath(wordcountJob, new Path(outputFolderStr));

            //6，提交，开始处理
            wordcountJob.waitForCompletion(false);

        } catch (IOException e) {
            e.printStackTrace();
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}
```

## 2，两张大表，reduce端连接
*reduce端连接其实就是二次排序的主要应用。我们还是拿用户和订单表的连接说明。假设用户表也很大，订单表也很大。那么就需要进行reduce端连接。具体如下：*

* 01，首先创建一个模型，这里因为有订单和用户，所以这个模型要稍微复杂一些。一个模型中需要能判别类型，然后重点重写比较方法，分几种情况：不同的用户cid是升序还是降序。相同用户的不同订单，订单号是升序还是降序。最重要的是两个模型一个是用户，一个是订单的时候，一定要把用户放在最前面，方便后续reduce中取出第一个就是用户。

* 02，然后是分组，相同的cid分到一组即可。然后在每组数据中取出第一个就是用户模型，剩下的就是这个用户下面的订单模型。明白了吧，具体看代码

### 01，IMJoinKey最重要，先看模型

```java
package im.ivanl001.bigData.Hadoop.A18_ReducerJoin;

import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-11-01 18:31
 * #description : 自定义key
 **/
public class IMJoinKey implements WritableComparable<IMJoinKey> {

    //0 customer, 1 order
    private int type;
    private int cid ;
    private int oid = -1;
    //这里地方要有一个初始化的值，因为最后要写入写出，不能为null，不然会报错
    private String userInfo = "";
    private String orderInfo = "";


    public int compareTo(IMJoinKey o) {

        int otherType = o.getType();
        int otherCid = o.getCid();
        int otherOid = o.getOid();

        String otherUserInfo = o.getUserInfo();
        String otherOrderInfo = o.getOrderInfo();

        if (this.cid == otherCid) {
            //说明是同一个用户的
            if (this.type == otherType) {
                //说明是同一个用户的不同订单
                return this.oid-otherOid;

            } else {
                //说明一个是用户，一个是订单，(因为类型不同)
                if (type == 0) {
                    //这里把用户放在最前面，方便后续直接取出组里面第一个进行操作。
                    return -1;//把用户放在前面
                }else {
                    return 1;//还是把用户放在前面
                }
            }
        }else{
            //说明是不同的用户的信息
            return this.cid - otherCid;
        }
    }

    public void write(DataOutput out) throws IOException {
        out.writeInt(type);
        out.writeInt(cid);
        out.writeInt(oid);
        out.writeUTF(userInfo);
        out.writeUTF(orderInfo);
    }

    public void readFields(DataInput in) throws IOException {
        this.type = in.readInt();
        this.cid = in.readInt();
        this.oid = in.readInt();
        this.userInfo = in.readUTF();
        this.orderInfo = in.readUTF();
    }


    @Override
    public String toString() {
        return "IMJoinKey{" +
                "type=" + type +
                ", cid=" + cid +
                ", oid=" + oid +
                ", userInfo='" + userInfo + '\'' +
                ", orderInfo='" + orderInfo + '\'' +
                '}';
    }

    //get和set方法为了简洁，我这里直接删除了。
}
```

### 02，IMJoinKeyComparator比较类

```java
package im.ivanl001.bigData.Hadoop.A18_ReducerJoin;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

/**
 * #author      : ivanl001
 * #creator     : 2018-11-01 18:58
 * #description :
 **/
public class IMJoinKeyComparator extends WritableComparator {

    protected IMJoinKeyComparator(){
        super(IMJoinKey.class, true);
    }

    @Override
    public int compare(WritableComparable a, WritableComparable b) {
        IMJoinKey joinKey01 = (IMJoinKey) a;
        IMJoinKey joinKey02 = (IMJoinKey) b;
        return joinKey01.compareTo(joinKey02);
    }
}
```

### 03，IMCidPartitioner分区类
```java
package im.ivanl001.bigData.Hadoop.A18_ReducerJoin;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Partitioner;

/**
 * #author      : ivanl001
 * #creator     : 2018-11-01 18:47
 * #description : 根据用户id，cid进行分区，决定这个用户的相关信息在哪个r上执行
 *               这里的输入就是mapper的输出
 **/
public class IMCidPartitioner extends Partitioner<IMJoinKey, NullWritable> {

    public int getPartition(IMJoinKey imJoinKey, NullWritable nullWritable, int numPartitions) {
        int cid = imJoinKey.getCid();
        System.out.println("======================="+numPartitions);
        return cid%numPartitions;
    }
}
```

### 04，IMCidGroupSeparator分组类
```java
package im.ivanl001.bigData.Hadoop.A18_ReducerJoin;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

/**
 * #author      : ivanl001
 * #creator     : 2018-11-01 18:50
 * #description : 根据cid进行分区，因为一个r上会有多个cid同时存在，那么这些cid如何进行进行就需要根据这个类
 **/
public class IMCidGroupSeparator extends WritableComparator {

    protected IMCidGroupSeparator(){
        super(IMJoinKey.class, true);
    }

    @Override
    public int compare(WritableComparable a, WritableComparable b) {

        IMJoinKey joinKey01 = (IMJoinKey) a;
        IMJoinKey joinKey02 = (IMJoinKey) b;

        return joinKey01.getCid()-joinKey02.getCid();
    }
}
```

### 05，IMJoinMapper
```java
package im.ivanl001.bigData.Hadoop.A18_ReducerJoin;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.IOException;

/**
 * #author      : ivanl001
 * #creator     : 2018-11-01 18:27
 * #description : 如果在reduce端join的话，也就相当于二次排序了
 **/
public class IMJoinMapper extends Mapper<LongWritable, Text, IMJoinKey, NullWritable> {

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

        //1, 首先需要根据输入判断是customer还是order
        FileSplit fileSplit = (FileSplit) context.getInputSplit();
        String path = fileSplit.getPath().toString();
        IMJoinKey joinKey = new IMJoinKey();

        String line = value.toString();

        if (path.contains("customers")) {

            //是用户
            joinKey.setType(0);
            String cid = line.substring(0, line.indexOf(","));
            String userInfo = line.substring(line.indexOf(",") + 1);

            joinKey.setCid(Integer.parseInt(cid));
            joinKey.setUserInfo(userInfo);

            System.out.println(joinKey);

        }else {
            //是订单
            joinKey.setType(1);
            String oid = line.substring(0, line.indexOf(","));
            String orderInfo = line.substring(0, line.lastIndexOf(","));
            String cid = line.substring(line.lastIndexOf(",") + 1);

            joinKey.setOid(Integer.parseInt(oid));
            joinKey.setCid(Integer.parseInt(cid));
            joinKey.setOrderInfo(orderInfo);

            System.out.println(joinKey);

        }

        //2, 直接把上面的信息写出即可
        context.write(joinKey, NullWritable.get());
    }
}
```

### 06，IMJoinReducer
```java
package im.ivanl001.bigData.Hadoop.A18_ReducerJoin;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.util.Iterator;

/**
 * #author      : ivanl001
 * #creator     : 2018-11-01 19:16
 * #description :
 **/
public class IMJoinReducer extends Reducer<IMJoinKey, NullWritable, Text, NullWritable> {

    @Override
    protected void reduce(IMJoinKey key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {

        Iterator<NullWritable> iterator = values.iterator();
        iterator.next();

        //因为排序规则默认是把用户放在第一个的，所以直接把第一个取出来即可
        int type_c = key.getType();
        int cid_c = key.getCid();
        String userInfo = key.getUserInfo();
        System.out.println("====================================================================================");
        System.out.println(key);

        int i = 0;
        while (iterator.hasNext()) {
            iterator.next();
            i += 1;

            int type_o = key.getType();
            int oid = key.getOid();
            int cid_o= key.getCid();
            String orderInfo = key.getOrderInfo();

            String outStr = cid_c + "----" + cid_o + "----" + userInfo + "----" + oid + "----" + orderInfo;
            System.out.println("++++++++++++++++++++++++++++++++++----"+i+"----++++++++++++++++++++++++++++++++++");
            System.out.println(key);
            context.write(new Text(outStr), NullWritable.get());
        }
    }
}
```

### 7,IMReducerJoinApp最后的app
```java
package im.ivanl001.bigData.Hadoop.A18_ReducerJoin;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

/**
 * #author      : ivanl001
 * #creator     : 2018-11-01 19:24
 * #description :
 **/
public class IMReducerJoinApp {

    public static void main(String[] args) throws Exception {

        //1，先判断参数，如果通过则取出参数
        if (args.length != 2) {
            System.out.println("参数个数有误！");
            return;
        }
        String inputFileStr = args[0];
        String outputFolderStr = args[1];


        //2，获取文件系统，并删除输出文件夹以免出错
        Configuration configuration = new Configuration();
        FileSystem fileSystem = FileSystem.get(configuration);
        fileSystem.delete(new Path(outputFolderStr));


        //3,创建作业, 并设置基本的选项
        Job maxTempJob = Job.getInstance(configuration);
        maxTempJob.setJobName("reduceJoinApp");
        maxTempJob.setJarByClass(IMReducerJoinApp.class);
        //因为采用的是序列文件，所以这里需要设置一下
        maxTempJob.setInputFormatClass(TextInputFormat.class);


        //4, 输入
        FileInputFormat.addInputPath(maxTempJob, new Path(inputFileStr));


        //5, 设置mapper
        maxTempJob.setMapperClass(IMJoinMapper.class);
        //这里其实是有点蠢的，其实在IMMaxTempMapper中已经指定了输出的key和value的类型了，但是这里还是需要指定一下，要不然就有可能不对
        maxTempJob.setMapOutputKeyClass(IMJoinKey.class);
        maxTempJob.setMapOutputValueClass(NullWritable.class);


        //6, 设置reducer
        //注意：如果有多个r的话(一般都会有多个r)，那么这样的就不属于全排序了，因为不同的年份会分布在不同的r中处理，也就会有多个文件输出
        maxTempJob.setReducerClass(IMJoinReducer.class);
        maxTempJob.setNumReduceTasks(2);



        //设置分区函数，自定义分区是ok的，那我们再试试用全排序加采样进行试试
        maxTempJob.setPartitionerClass(IMCidPartitioner.class);


        //排序和分组都是在mapper之后进行的，先进行排序，排序之后在进行分组



        //设置排序
        maxTempJob.setSortComparatorClass(IMJoinKeyComparator.class);

        //设置分组
        maxTempJob.setGroupingComparatorClass(IMCidGroupSeparator.class);




        //------------------------注意：采样要放在分区之后，否则会报错什么找不到分区文件，这种问题其实感觉应该是bug，不然为啥要放在后面，本来采样就是在r之前的啊------------------------------------
        /*

        //7.1, 设置分区函数,这里还是不能用自己定义的分区函数，因为你用自己定义的分区函数，采样的意义也就没有了
        //maxTempJob.setPartitionerClass(IMYearPatitioner.class);
        maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);
        //如果需要指定位置的话，需要注意一下：设置的时候使用configuration是不能直接使用的，而是需要从maxTempJob.getConfiguration()取出来使用，因为job里面是重新拷贝了一份
        //TotalOrderPartitioner.setPartitionFile(maxTempJob.getConfiguration(), new Path("/Users/ivanl001/Desktop/bigData/output005/partitioner.lst"));

        //7.2, 设置采样
        //说明一下，因为采样是对key进行采样的，所以只好用seqfile，要不然比如说用的是文本文件格式， key是偏移量，采样就没有什么意义了
        //这句话暂时不知道为什么要设置，先注释
        //maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);
        InputSampler.Sampler<IntWritable, IntWritable> sampler = new InputSampler.RandomSampler<IntWritable, IntWritable>(0.1, 10000, 10);
        InputSampler.writePartitionFile(maxTempJob, sampler);

        //----------呀呀呀呀，采样真的很好用哈，哈哈哈哈---------------

        *//*Configuration conf = maxTempJob.getConfiguration();
        String partitionFile = TotalOrderPartitioner.getPartitionFile(conf);
        URI partitionUri = new URI(partitionFile);
        maxTempJob.addCacheFile(partitionUri);*//*
         */

        //7，输出
        FileOutputFormat.setOutputPath(maxTempJob, new Path(outputFolderStr));

        //8, 提交，开始处理
        maxTempJob.waitForCompletion(false);
    }
}
```

!!! 完结撒花  !!!