l 分词机制

| Character Filter | 对原始文本进行处理     | 例：去除html标签、特殊字符等             |
| ---------------- | ---------------------- | ---------------------------------------- |
| Tokenizer        | 将原始文本进行分词     | 例：培训机构-->培训，机构                |
| Token Filters    | 分词后的关键字进行加工 | 例：转小写、删除语气词、近义词和同义词等 |



```shell
# 直接指定分词器分词
POST _analyze
{
  "analyzer": "standard",
  "text": "ivanl001 is the king of world!"
}


# 使用某个索引的分词器分词
POST /azazie/_analyze
{
  "field": "name",
  "text": "ivanl001 is the king of world!"
}


# 过滤，tokenizer和analyzer还暂时分不清楚
POST _analyze
{
  
  "tokenizer": "standard",
  "filter": ["lowercase"],
  "text": "ivanl001 IS the KING of world!"
  
}
```







l Elasticsearch自带的分词器

| 分词器（Analyzer） | 特点                                   |
| ------------------ | -------------------------------------- |
| Standard（es默认） | 支持多语言，按词切分并做小写处理       |
| Simple             | 按照非字母切分，小写处理               |
| Whitespace         | 按照空格来切分                         |
| Stop               | 去除语气助词，如the、an、的、这等      |
| Keyword            | 不分词                                 |
| Pattern            | 正则分词，默认\w+,即非字词符号做分割符 |
| Language           | 常见语言的分词器（30+）                |

l 中文分词

| 分词器名称 | 介绍                                   | 特点                               | 地址                                                    |
| ---------- | -------------------------------------- | ---------------------------------- | ------------------------------------------------------- |
| IK         | 实现中英文单词切分                     | 自定义词库                         | https://github.com/medcl/elasticsearch-analysis-ik      |
| Jieba      | python流行分词系统，支持分词和词性标注 | 支持繁体、自定义、并行分词         | http://github.com/sing1ee/elasticsearch-jieba-plugin    |
| Hanlp      | 由一系列模型于算法组成的java工具包     | 普及自然语言处理在生产环境中的应用 | https://github.com/hankcs/HanLP                         |
| THULAC     | 清华大学中文词法分析工具包             | 具有中文分词和词性标注功能         | https://github.com/microbun/elasticsearch-thulac-plugin |



l 自定义分词api

 ```shell
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my":{
          "tokenizer":"punctuation",
          "type":"custom",
          "char_filter":["emoticons"],
          "filter":["lowercase","english_stop"]
        }
      },
      "tokenizer": {
        "punctuation":{
          "type":"pattern",
          "pattern":"[.,!?]"
        }
      },
      "char_filter": {
        "emoticons":{
          "type":"mapping",
          "mappings":[
              ":)=>_happy_",
              ":(=>_sad_"
            ]
        }
      },
      "filter": {
        "english_stop":{
          "type":"stop",
          "stopwords":"_english_"
        }
      }
    }
  }
 ```





l IK分词器测试

IK提供了两个分词算法ik_smart和ik_max_word，其中ik_smart 为最少切分，ik_max_word为最细粒度划分









自定义分词器

```shell
PUT my_analyzer
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my":{
          "tokenizer":"punctuation",
          "type":"custom",
          "char_filter":["emoticons"],
          "filter":["lowercase","english_stop"]
        }
      },
      
      "tokenizer": {
        "punctuation":{
          "type":"pattern",
          "pattern":"[.,!?]"
        }
      },
      
      "char_filter": {
        "emoticons":{
          "type":"mapping",
          "mappings":[
              ":)=>_happy_",
              ":(=>_sad_"
            ]
        }
      },
      
      "filter": {
        "english_stop":{
          "type":"stop",
          "stopwords":"_english_"
        }
      }
      
    }
  }
}

# 如下语句测试
POST my_analyzer/_analyze
{
  "analyzer": "my",
  "text":"l'm a :) person,and you?"
}

```

